# -*- coding: utf-8 -*-
"""Data Analysis (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ORUPCbUANCyuK82Ky9llOT9jksnpCgcW
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %pip install opencv-python
import cv2

"""# DUST Dataset

The DUTS dataset is a widely used benchmark in salient object detection, a computer vision task focused on identifying and segmenting the most conspicuous objects within an image. Introduced by Wang et al. in 2017, DUTS is notable for its large scale and challenging scenarios. It comprises two subsets: a training set with 10,553 images and a test set with 5,019 images. The training images are sourced from the ImageNet DET training and validation sets, while the test images are drawn from the ImageNet DET test set and the SUN dataset. Each image in DUTS is accompanied by accurate pixel-level ground truth annotations, manually labeled by 50 subjects, facilitating precise evaluation of saliency detection algorithms.

[DUST Dataset](https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Learning_to_Detect_CVPR_2017_paper.html)

# 1. Data Analysis
The purpose of data analysis in this context is to understand the characteristics and quality of the dataset, ensuring it is suitable for the development and evaluation of salient object detection models. First, we load the data to access its contents and then check its format to verify compatibility and consistency. Next, we examine the dimensions of the images and corresponding masks to ensure they align correctly, as mismatched dimensions could affect model training. Analyzing pixel values in both images and masks provides insights into data distribution and highlights any anomalies or inconsistencies. Computing the average dimensions helps identify typical sizes, guiding preprocessing steps like resizing or cropping.Computing the Standard deviation of salient coverage across masks provides a sense of object prevalence in the dataset, informing model expectations. Checking unique pixel values in masks ensures that labels are correctly encoded, which is crucial for accurate training.  Finally, detecting outliers helps identify problematic samples that could skew results. Each step is essential to prepare the dataset effectively, validate its integrity, and tailor it to the requirements of salient object detection tasks. To ensure thoroughness, we should also check for missing files, incomplete annotations, and overall dataset balance.

### Library
"""

import os
import random
import numpy as np
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import Counter

train_normalized_image_path = "/content/drive/MyDrive/DUST-NORMALIZED/DUTS-TR/DUTS-TR-Normalized-Images"
train_normalized_mask_path = "/content/drive/MyDrive/DUST-NORMALIZED/DUTS-TR/DUTS-TR-Normalized-Masks"

test_normalized_image_path = "/content/drive/MyDrive/DUST-NORMALIZED/DUTS-TE/DUTS-TE-Normalized-Images"
test_normalized_mask_path = "/content/drive/MyDrive/DUST-NORMALIZED/DUTS-TE/DUTS-TE-Normalized-Masks"

"""### 1.1 Load Dataset"""

test_image_path = "/content/drive/MyDrive/DUTS/DUTS-TE/DUTS-TE-Image"
test_mask_path = "/content/drive/MyDrive/DUTS/DUTS-TE/DUTS-TE-Mask"

test_images = os.listdir(test_image_path)
test_masks = os.listdir(test_mask_path)

print(f"Number of Test images: {len(test_images)}")
print(f"Number of Test masks: {len(test_masks)}")

not_missing_masks = [img for img in test_images if img in test_masks]
if not_missing_masks:
    print(f"Missing masks for {len(not_missing_masks)} images.")
else:
    print("All Test images have corresponding masks.")

train_image_path = "/content/drive/MyDrive/DUTS/DUTS-TR/DUTS-TR-Image"
train_mask_path = "/content/drive/MyDrive/DUTS/DUTS-TR/DUTS-TR-Mask"

train_images = os.listdir(train_image_path)
train_masks = os.listdir(train_mask_path)

print(f"Number of images: {len(train_images)}")
print(f"Number of masks: {len(train_masks)}")

not_missing_masks = [img for img in train_images if img in train_masks]
if not_missing_masks:
    print(f"Missing masks for {len(not_missing_masks)} images.")
else:
    print("All images have corresponding masks.")

"""### 1.2 Check file format"""

image_formats = {os.path.splitext(file)[1] for file in train_images}
mask_formats = {os.path.splitext(file)[1] for file in train_masks}
print(f"Train Image formats: {image_formats}")
print(f"Tain Mask formats: {mask_formats}")

image_formats = {os.path.splitext(file)[1] for file in test_images}
mask_formats = {os.path.splitext(file)[1] for file in test_masks}
print(f"Test Image formats: {image_formats}")
print(f"Test Mask formats: {mask_formats}")

"""### 1.3 Image and Mask Dimensions"""

print("\nSample Image and Mask Dimensions:")
for i in range(min(5, len(train_images))):
    img = Image.open(os.path.join(train_image_path, train_images[i]))
    mask = Image.open(os.path.join(train_mask_path, train_masks[i]))
    print(f"Train Image {train_images[i]}: {img.size}, Mode: {img.mode}")
    print(f"Train Mask {train_masks[i]}: {mask.size}, Mode: {mask.mode}")

print("\nSample Image and Mask Dimensions:")
for i in range(min(5, len(test_images))):
    img = Image.open(os.path.join(test_image_path, test_images[i]))
    mask = Image.open(os.path.join(test_mask_path, test_masks[i]))
    print(f"Test Image {test_images[i]}: {img.size}, Mode: {img.mode}")
    print(f"Test Mask {test_masks[i]}: {mask.size}, Mode: {mask.mode}")

"""### 1.4 Display an image and mask"""

train_sample_image = Image.open("/content/drive/MyDrive/DUTS/DUTS-TR/DUTS-TR-Image/ILSVRC2012_test_00000004.jpg")
train_sample_mask = Image.open("/content/drive/MyDrive/DUTS/DUTS-TR/DUTS-TR-Mask/ILSVRC2012_test_00000004.png")

test_sample_image = Image.open("/content/drive/MyDrive/DUTS/DUTS-TE/DUTS-TE-Image/ILSVRC2012_test_00000003.jpg")
test_sample_mask = Image.open("/content/drive/MyDrive/DUTS/DUTS-TE/DUTS-TE-Mask/ILSVRC2012_test_00000003.png")

plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
plt.imshow(train_sample_image)
plt.title('Train Image')
plt.axis('off')

plt.subplot(2, 2, 2)
plt.imshow(train_sample_mask, cmap='gray')
plt.axis('off')

plt.subplot(2, 2, 3)
plt.imshow(test_sample_image)
plt.title('Test Image')
plt.axis('off')

plt.subplot(2, 2, 4)
plt.imshow(test_sample_mask, cmap='gray')
plt.title('Test Mask')
plt.axis('off')

plt.tight_layout()
plt.show()

"""### 1.5 Analyze pixel values in images"""

image_array = np.array(train_sample_image)

plt.figure(figsize=(8, 6))
plt.hist(image_array.ravel(), bins=256, color='blue', alpha=0.7)
plt.title("Pixel Value Distribution in Train Image")
plt.xlabel("Pixel Value")
plt.ylabel("Frequency")
plt.show()

timage_array = np.array(test_sample_image)

plt.figure(figsize=(8, 6))
plt.hist(timage_array.ravel(), bins=256, color='blue', alpha=0.7)
plt.title("Pixel Value Distribution in Test Image")
plt.xlabel("Pixel Value")
plt.ylabel("Frequency")
plt.show()

"""### 1.5 Analyze pixel values in masks"""

mask_array = np.array(train_sample_mask)
print(mask_array.min(), mask_array.max())
print("____________")
plt.hist(mask_array.ravel(), bins=50, color='gray', alpha=0.7)
plt.title("Pixel Value Distribution in Saliency Train Masks")
plt.show()

tmask_array = np.array(test_sample_mask)
print(tmask_array.min(), tmask_array.max())
print("____________")
plt.hist(tmask_array.ravel(), bins=50, color='gray', alpha=0.7)
plt.title("Pixel Value Distribution in Saliency Test Masks")
plt.show()

"""#### 1.6 Inspect Data Distribution"""

print(Counter(mask_array.ravel()))
print("____________________________________________________")

fig, ax = plt.subplots(1, 2, figsize=(10, 5))
ax[0].imshow(train_sample_image)
ax[0].set_title("Image")
ax[1].imshow(train_sample_mask, cmap='gray')
ax[1].set_title("Saliency Mask")
plt.show()

print(Counter(tmask_array.ravel()))
print("____________________________________________________")

fig, ax = plt.subplots(1, 2, figsize=(10, 5))
ax[0].imshow(test_sample_image)
ax[0].set_title("Image")
ax[1].imshow(test_sample_mask, cmap='gray')
ax[1].set_title("Saliency Mask")
plt.show()

"""### 1.7 Compute Average and standard deviation of salient coverage"""

total_width, total_height = 0, 0
total_mask_width, total_mask_height = 0, 0
salient_area_ratios = []
unique_pixel_values = Counter()
use_sample = False
sample_size = 1000

images = sorted(os.listdir(train_image_path))
masks = sorted(os.listdir(train_mask_path))

# Sampling logic
if use_sample:
    sample_indices = random.sample(range(len(images)), sample_size)
    images = [images[i] for i in sample_indices]
    masks = [masks[i] for i in sample_indices]


print("Processing images and masks...")
for img_file, mask_file in tqdm(zip(images, masks), total=len(images)):
    try:
        img = Image.open(os.path.join(train_image_path, img_file))
        mask = Image.open(os.path.join(train_mask_path, mask_file))

        total_width += img.width
        total_height += img.height

        total_mask_width += mask.width
        total_mask_height += mask.height

        mask_array = np.array(mask, dtype=np.uint8)
        unique_pixel_values.update(mask_array.ravel())

        non_zero_pixels = np.count_nonzero(mask_array)
        total_pixels = mask_array.size
        salient_area_ratios.append(non_zero_pixels / total_pixels)
    except Exception as e:
        print(f"Error processing {img_file} or {mask_file}: {e}")
        continue

avg_width = total_width / len(images)
avg_height = total_height / len(images)
avg_mask_width = total_mask_width / len(masks)
avg_mask_height = total_mask_height / len(masks)
avg_salient_area = np.mean(salient_area_ratios) * 100
std_salient_area = np.std(salient_area_ratios) * 100

print("\nTrain Dataset Statistics:")
print(f"Number of Images & Masks: {len(images)}")
print(f"Average Image Dimensions: {avg_width:.2f} x {avg_height:.2f}")
print(f"Average Mask Dimensions: {avg_mask_width:.2f} x {avg_mask_height:.2f}")
print(f"Average Salient Area: {avg_salient_area:.2f}%")
print(f"Standard Deviation of Salient Area: {std_salient_area:.2f}%")

total_width, total_height = 0, 0
total_mask_width, total_mask_height = 0, 0
salient_area_ratios = []
unique_pixel_values = Counter()
use_sample = False
sample_size = 1000

images = sorted(os.listdir(test_image_path))
masks = sorted(os.listdir(test_mask_path))

if use_sample:
    sample_indices = random.sample(range(len(images)), sample_size)
    images = [images[i] for i in sample_indices]
    masks = [masks[i] for i in sample_indices]


print("Processing images and masks...")
for img_file, mask_file in tqdm(zip(images, masks), total=len(images)):
    try:
        img = Image.open(os.path.join(test_image_path, img_file))
        mask = Image.open(os.path.join(test_mask_path, mask_file))

        total_width += img.width
        total_height += img.height

        total_mask_width += mask.width
        total_mask_height += mask.height

        mask_array = np.array(mask, dtype=np.uint8)
        unique_pixel_values.update(mask_array.ravel())

        non_zero_pixels = np.count_nonzero(mask_array)
        total_pixels = mask_array.size
        salient_area_ratios.append(non_zero_pixels / total_pixels)
    except Exception as e:
        print(f"Error processing {img_file} or {mask_file}: {e}")
        continue

avg_width = total_width / len(images)
avg_height = total_height / len(images)
avg_mask_width = total_mask_width / len(masks)
avg_mask_height = total_mask_height / len(masks)
avg_salient_area = np.mean(salient_area_ratios) * 100
std_salient_area = np.std(salient_area_ratios) * 100

print("\nTest Dataset Statistics:")
print(f"Number of Images & Masks: {len(images)}")
print(f"Average Image Dimensions: {avg_width:.2f} x {avg_height:.2f}")
print(f"Average Mask Dimensions: {avg_mask_width:.2f} x {avg_mask_height:.2f}")
print(f"Average Salient Area: {avg_salient_area:.2f}%")
print(f"Standard Deviation of Salient Area: {std_salient_area:.2f}%")

"""### 1.8 Unique pixel values in masks"""

def unique_pixel_values (mask_array, threshold = 128):
  binary_mask = (mask_array > threshold).astype(int)  # Convert to 0 and 1
  unique_pixel_values = {value: (binary_mask == value).sum() for value in np.unique(binary_mask)}
  print(f"Unique Pixel Values in Binary Mask: {sorted(unique_pixel_values.keys())}")
  return binary_mask

mask_array = np.array(train_sample_mask)
plt.imshow(mask_array, cmap='gray')
plt.colorbar()
plt.title("Train Mask Visualization")
plt.show()

plt.hist(mask_array.ravel(), bins=256, color='gray', alpha=0.7)
plt.title("Train Pixel Value Distribution in Mask")
plt.xlabel("Pixel Value")
plt.ylabel("Frequency")
plt.show()

train_binary_mask = unique_pixel_values(mask_array)

tmask_array = np.array(test_sample_mask)
plt.imshow(tmask_array, cmap='gray')
plt.colorbar()
plt.title("Test Mask Visualization")
plt.show()

plt.hist(tmask_array.ravel(), bins=256, color='gray', alpha=0.7)
plt.title("Test Pixel Value Distribution in Mask")
plt.xlabel("Pixel Value")
plt.ylabel("Frequency")
plt.show()

test_binary_mask = unique_pixel_values(tmask_array)

"""### 1.9 Detecting outlier"""

image_path = train_image_path
mask_path = train_mask_path

images = sorted(os.listdir(image_path))
masks = sorted(os.listdir(mask_path))

print("Train Calculating dimensions...")
dimensions = [(Image.open(os.path.join(image_path, img)).size) for img in tqdm(images)]
widths, heights = zip(*dimensions)

print("Train Calculating salient area ratios...")
salient_area_ratios = []
for mask_file in tqdm(masks):
    mask = Image.open(os.path.join(mask_path, mask_file))
    mask_array = np.array(mask, dtype=np.uint8)
    non_zero_pixels = np.count_nonzero(mask_array)
    total_pixels = mask_array.size
    salient_area_ratios.append(non_zero_pixels / total_pixels)

plt.figure(figsize=(10, 5))

# image widths
plt.subplot(1, 2, 1)
plt.boxplot(widths)
plt.title('Image Widths')
plt.xlabel('Widths')

# image heights
plt.subplot(1, 2, 2)
plt.boxplot(heights)
plt.title('Image Heights')
plt.xlabel('Heights')

plt.show()

# salient area ratios
plt.figure(figsize=(5, 5))
plt.boxplot(salient_area_ratios)
plt.title('Salient Area Coverage')
plt.ylabel('Salient Area Ratio')
plt.show()

"""#### Detect outliers based on IQR"""

def detect_outliers(data):
    q1 = np.percentile(data, 25)  # Q1
    q3 = np.percentile(data, 75)  # Q3
    iqr = q3 - q1  #
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = [x for x in data if x < lower_bound or x > upper_bound]
    return outliers, lower_bound, upper_bound

# Detect outliers for widths, heights, and salient area ratios
width_outliers, width_lb, width_ub = detect_outliers(widths)
height_outliers, height_lb, height_ub = detect_outliers(heights)
salient_outliers, salient_lb, salient_ub = detect_outliers(salient_area_ratios)

print("\nOutlier Detection Results:")
print(f"Width Outliers: {width_outliers}")
print(f"Height Outliers: {height_outliers}")
print(f"Salient Area Outliers: {salient_outliers}")

print("\nBounds:")
print(f"Width Bounds: {width_lb:.2f}, {width_ub:.2f}")
print(f"Height Bounds: {height_lb:.2f}, {height_ub:.2f}")
print(f"Salient Area Bounds: {salient_lb:.4f}, {salient_ub:.4f}")

image_path = test_image_path
mask_path = test_mask_path

images = sorted(os.listdir(image_path))
masks = sorted(os.listdir(mask_path))

print("Test Calculating dimensions...")
dimensions = [(Image.open(os.path.join(image_path, img)).size) for img in tqdm(images)]
widths, heights = zip(*dimensions)

print("Test Calculating salient area ratios...")
salient_area_ratios = []
for mask_file in tqdm(masks):
    mask = Image.open(os.path.join(mask_path, mask_file))
    mask_array = np.array(mask, dtype=np.uint8)
    non_zero_pixels = np.count_nonzero(mask_array)
    total_pixels = mask_array.size
    salient_area_ratios.append(non_zero_pixels / total_pixels)

plt.figure(figsize=(10, 5))

# image widths
plt.subplot(1, 2, 1)
plt.boxplot(widths)
plt.title('Image Widths')
plt.xlabel('Widths')

# image heights
plt.subplot(1, 2, 2)
plt.boxplot(heights)
plt.title('Image Heights')
plt.xlabel('Heights')

plt.show()

# salient area ratios
plt.figure(figsize=(5, 5))
plt.boxplot(salient_area_ratios)
plt.title('Salient Area Coverage')
plt.ylabel('Salient Area Ratio')
plt.show()

def detect_outliers(data):
    q1 = np.percentile(data, 25)  # Q1
    q3 = np.percentile(data, 75)  # Q3
    iqr = q3 - q1  #
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = [x for x in data if x < lower_bound or x > upper_bound]
    return outliers, lower_bound, upper_bound

# Detect outliers for widths, heights, and salient area ratios
width_outliers, width_lb, width_ub = detect_outliers(widths)
height_outliers, height_lb, height_ub = detect_outliers(heights)
salient_outliers, salient_lb, salient_ub = detect_outliers(salient_area_ratios)

print("\nOutlier Detection Results:")
print(f"Width Outliers: {width_outliers}")
print(f"Height Outliers: {height_outliers}")
print(f"Salient Area Outliers: {salient_outliers}")

print("\nBounds:")
print(f"Width Bounds: {width_lb:.2f}, {width_ub:.2f}")
print(f"Height Bounds: {height_lb:.2f}, {height_ub:.2f}")
print(f"Salient Area Bounds: {salient_lb:.4f}, {salient_ub:.4f}")

"""# 2.Data Preprocessing
After conducting data analysis, the next step is data preprocessing, which ensures the dataset is optimized for model training. First, images and masks will be resized to the average dimensions computed during analysis to standardize input sizes for the neural network, reducing computational overhead and ensuring uniformity across the dataset. Next, pixel values will be normalized to a range of 0 to 1 (or standardized), which improves model convergence by ensuring features are on a similar scale. Data augmentation techniques like rotation, flipping, and scaling will be applied to artificially increase dataset diversity, improving model generalization. For imbalanced datasets, techniques such as oversampling minority classes or applying class-weighted loss will be used to handle disparities in class representation. Finally, all data will be converted to tensors, as this format is required for PyTorch or TensorFlow models, enabling efficient processing and computation on GPUs. Each preprocessing step is essential to address specific dataset issues, enhance model performance, and prepare data for effective learning.

### 2.1 Resizing Images and Masks
"""

#train_image_path = "/content/drive/MyDrive/DUTS/DUTS-TR/DUTS-TR-Image"
#train_mask_path = "/content/drive/MyDrive/DUTS/DUTS-TR/DUTS-TR-Mask"

image_path = train_image_path
mask_path = train_mask_path

if not os.path.exists(image_path):
    print(f"Image file not found: {image_path}")
if not os.path.exists(mask_path):
    print(f"Mask file not found: {mask_path}")

train_images = sorted(os.listdir(image_path))
train_masks = sorted(os.listdir(mask_path))

train_resized_image_path = "/content/drive/MyDrive/DUST-RESIZED/DUTS-TR/DUTS-TR-Resized-Images"
train_resized_mask_path = "/content/drive/MyDrive/DUST-RESIZED/DUTS-TR/DUTS-TR-Resized-Masks"

os.makedirs(train_resized_image_path, exist_ok=True)
os.makedirs(train_resized_mask_path, exist_ok=True)

timage_path = test_image_path
tmask_path = test_mask_path

if not os.path.exists(timage_path):
    print(f"Image file not found: {timage_path}")
if not os.path.exists(tmask_path):
    print(f"Mask file not found: {tmask_path}")

test_images = sorted(os.listdir(timage_path))
test_masks = sorted(os.listdir(tmask_path))

test_resized_image_path = "/content/drive/MyDrive/DUST-RESIZED/DUTS-TE/DUTS-TE-Resized-Images"
test_resized_mask_path = "/content/drive/MyDrive/DUST-RESIZED/DUTS-TE/DUTS-TE-Resized-Masks"

os.makedirs(test_resized_image_path, exist_ok=True)
os.makedirs(test_resized_mask_path, exist_ok=True)

def resize_and_save_image(img_path, msk_path, target_size, image_save_path, mask_save_path):
    image = cv2.imread(img_path)
    mask = cv2.imread(msk_path, cv2.IMREAD_GRAYSCALE)

    if image is None or mask is None:
        raise ValueError(f"Failed to load image or mask: {img_path}, {msk_path}")

    image_resized = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)
    mask_resized = cv2.resize(mask, target_size, interpolation=cv2.INTER_NEAREST)

    img_filename = os.path.basename(img_path)
    mask_filename = os.path.basename(msk_path)

    resized_img_path = os.path.join(image_save_path, img_filename)
    resized_mask_path = os.path.join(mask_save_path, mask_filename)

    cv2.imwrite(resized_img_path, image_resized)
    cv2.imwrite(resized_mask_path, mask_resized)

    return resized_img_path, resized_mask_path

train_images_resized = []
train_masks_resized = []

print("Resizing and saving train images and masks...")
for img_file, mask_file in tqdm(zip(train_images, train_masks), total=len(train_images)):
    img_path = os.path.join(image_path, img_file)
    msk_path = os.path.join(mask_path, mask_file)

    try:
        resized_img_path, resized_mask_path = resize_and_save_image(
            img_path, msk_path, target_size=(372, 323),
            image_save_path=train_resized_image_path, mask_save_path=train_resized_mask_path
        )
        train_images_resized.append(resized_img_path)
        train_masks_resized.append(resized_mask_path)
    except Exception as e:
        print(f"Error resizing and saving: {e}")

print(f"Number of Train Images Resized and Saved: {len(train_images_resized)}")
print(f"Number of Train Masks Resized and Saved: {len(train_masks_resized)}")

test_images_resized = []
test_masks_resized = []

print("Resizing and saving test images and masks...")
for img_file, mask_file in tqdm(zip(test_images, test_masks), total=len(test_images)):
    img_path = os.path.join(timage_path, img_file)
    msk_path = os.path.join(tmask_path, mask_file)

    try:
        resized_img_path, resized_mask_path = resize_and_save_image(
            img_path, msk_path, target_size=(382, 321),
            image_save_path=test_resized_image_path, mask_save_path=test_resized_mask_path
        )
        test_images_resized.append(resized_img_path)
        test_masks_resized.append(resized_mask_path)
    except Exception as e:
        print(f"Error resizing and saving: {e}")

print(f"Number of Test Images Resized and Saved: {len(test_images_resized)}")
print(f"Number of Test Masks Resized and Saved: {len(test_masks_resized)}")

"""### 2.2 Normalization"""

nimage_path = "/content/drive/MyDrive/DUST-RESIZED/DUTS-TR/DUTS-TR-Resized-Images"
nmask_path = "/content/drive/MyDrive/DUST-RESIZED/DUTS-TR/DUTS-TR-Resized-Masks"

if not os.path.exists(nimage_path):
    print(f"Image file not found: {nimage_path}")
if not os.path.exists(nmask_path):
    print(f"Mask file not found: {nmask_path}")

ntrain_images = sorted(os.listdir(nimage_path))
ntrain_masks = sorted(os.listdir(nmask_path))
#train_images_resized
#train_masks_resized

train_normalized_image_path = "/content/drive/MyDrive/DUST-NORMALIZED/DUTS-TR/DUTS-TR-Normalized-Images"
train_normalized_mask_path = "/content/drive/MyDrive/DUST-NORMALIZED/DUTS-TR/DUTS-TR-Normalized-Masks"

os.makedirs(train_normalized_image_path, exist_ok=True)
os.makedirs(train_normalized_mask_path, exist_ok=True)

train_images_normalized = []
train_masks_normalized = []

tnimage_path = "/content/drive/MyDrive/DUST-RESIZED/DUTS-TE/DUTS-TE-Resized-Images"
tnmask_path = "/content/drive/MyDrive/DUST-RESIZED/DUTS-TE/DUTS-TE-Resized-Masks"
##
if not os.path.exists(tnimage_path):
    print(f"Image file not found: {tnimage_path}")
if not os.path.exists(tnmask_path):
    print(f"Mask file not found: {tnmask_path}")
##
ntest_images = sorted(os.listdir(tnimage_path))
ntest_masks = sorted(os.listdir(tnmask_path))
#test_images_resized
#test_masks_resized

test_normalized_image_path = "/content/drive/MyDrive/DUST-NORMALIZED/DUTS-TE/DUTS-TE-Normalized-Images"
test_normalized_mask_path = "/content/drive/MyDrive/DUST-NORMALIZED/DUTS-TE/DUTS-TE-Normalized-Masks"

os.makedirs(test_normalized_image_path, exist_ok=True)
os.makedirs(test_normalized_mask_path, exist_ok=True)

test_images_normalized = []
test_masks_normalized = []

def normalize_and_save_image(img_path, msk_path, image_save_path, mask_save_path):
    image = cv2.imread(img_path)
    mask = cv2.imread(msk_path, cv2.IMREAD_GRAYSCALE)

    if image is None or mask is None:
        raise ValueError(f"Failed to load image or mask: {img_path}, {msk_path}")

    image_normalized = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)

    mask_normalized = mask / 255.0

    img_filename = os.path.basename(img_path)
    mask_filename = os.path.basename(msk_path)

    normalized_img_path = os.path.join(image_save_path, img_filename)
    normalized_mask_path = os.path.join(mask_save_path, mask_filename)

    image_to_save = (image_normalized * 255).astype(np.uint8)
    mask_to_save = (mask_normalized * 255).astype(np.uint8)

    cv2.imwrite(normalized_img_path, image_to_save)
    cv2.imwrite(normalized_mask_path, mask_to_save)

    return normalized_img_path, normalized_mask_path

print("Normalizing and saving train images and masks...")
for img_file, mask_file in tqdm(zip(ntrain_images, ntrain_masks), total=len(ntrain_images)):
    img_path = os.path.join(image_path, img_file)
    msk_path = os.path.join(mask_path, mask_file)

    try:
        normalized_img_path, normalized_mask_path = normalize_and_save_image(
            img_path, msk_path,
            image_save_path=train_normalized_image_path, mask_save_path=train_normalized_mask_path
        )
    except Exception as e:
        print(f"Error normalizing and saving: {e}")

# Normalize and save test images and masks
print("Normalizing and saving test images and masks...")
for img_file, mask_file in tqdm(zip(ntest_images, ntest_masks), total=len(ntest_images)):
    img_path = os.path.join(timage_path, img_file)
    msk_path = os.path.join(tmask_path, mask_file)

    try:
        normalized_img_path, normalized_mask_path = normalize_and_save_image(
            img_path, msk_path,
            image_save_path=test_normalized_image_path, mask_save_path=test_normalized_mask_path
        )
    except Exception as e:
        print(f"Error normalizing and saving: {e}")

print("Normalization completed.")

"""### 2.3 Handling Class Imbalance
To check for class imbalance in a dataset, one can analyze the distribution of class labels by counting the occurrences of each class across the images. Class imbalance is determined by analyzing the masks because they contain the class labels for the foreground and background. The images themselves do not have labels, so they are not considered in the imbalance check. In the case of object saliency detection, this would involve checking the frequency of each pixel label, such as distinguishing between the salient object and the background. Imbalanced data can lead to biased model predictions, where the model favors the majority class, which typically results in poor performance for the minority class. This imbalance needs to be addressed during training to prevent the model from being evaluated based on this bias. Common techniques for handling imbalanced data include oversampling the minority class, undersampling the majority class, or assigning class weights to penalize the model more for misclassifying the minority class. Additionally, data augmentation can be employed to increase the representation of the minority class, making the model more robust and ensuring better generalization.
"""

def check_class_imbalance(masks):
    total_background = 0
    total_object = 0

    for mask in masks:
        #2.3.1 Flatten the Mask:
        mask_flat = mask.ravel()

        #2.3.2 Count the Unique Pixel Values
        pixel_counts = Counter(mask_flat)

        total_background += pixel_counts.get(0, 0)
        total_object += pixel_counts.get(1, 0)

    #2.3.3 Compute the Ratio
    total_pixels = total_background + total_object
    background_ratio = total_background / total_pixels
    object_ratio = total_object / total_pixels

    return background_ratio, object_ratio, total_background, total_object

def load_images_from_path(image_path):
    image_files = sorted([os.path.join(image_path, file) for file in os.listdir(image_path) if file.endswith(('.png', '.jpg', '.jpeg'))])
    images = [cv2.imread(img_file) for img_file in image_files]
    return images

def check_class_imbalance_batch(masks_generator):
    total_background = 0
    total_object = 0

    for masks_batch in masks_generator:
        for mask in masks_batch:
            mask_flat = mask.flatten()
            total_background += np.sum(mask_flat == 0)
            total_object += np.sum(mask_flat > 0)

    total_pixels = total_background + total_object
    if total_pixels == 0:
        return 0, 0, total_background, total_object  # Avoid division by zero

    background_ratio = total_background / total_pixels
    object_ratio = total_object / total_pixels

    return background_ratio, object_ratio, total_background, total_object

def load_images_in_batches(path, batch_size=100, grayscale=False):
    filenames = os.listdir(path)
    for i in range(0, len(filenames), batch_size):
        batch = filenames[i:i + batch_size]
        images = []
        for filename in batch:
            img_path = os.path.join(path, filename)
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE if grayscale else cv2.IMREAD_COLOR)
            if img is None:
                print(f"Warning: Could not read image {img_path}")
                continue
            images.append(img)
        yield images

"""**Balanced Dataset:**

If the ratio between background and object pixels is approximately equal (e.g., 50-50), the dataset is balanced.

**Imbalanced Dataset:**

If the ratio between background and object pixels is skewed (e.g., 90-10), the dataset is imbalanced.
"""

batch_size = 100

train_masks_generator = load_images_in_batches(train_normalized_mask_path, batch_size=batch_size, grayscale=True)
test_masks_generator = load_images_in_batches(test_normalized_mask_path, batch_size=batch_size, grayscale=True)

background_ratio, object_ratio, total_background, total_object = check_class_imbalance_batch(train_masks_generator)
print(f"Training Data - Background Ratio: {background_ratio:.4f}, Object Ratio: {object_ratio:.4f}")
print(f"Total Background Pixels: {total_background}, Total Object Pixels: {total_object}")

background_ratio, object_ratio, total_background, total_object = check_class_imbalance_batch(test_masks_generator)
print(f"Testing Data - Background Ratio: {background_ratio:.4f}, Object Ratio: {object_ratio:.4f}")
print(f"Total Background Pixels: {total_background}, Total Object Pixels: {total_object}")

"""### 2.4 Data Augmentation
Data augmentation is crucial when the dataset is small, to avoid overfitting, or when there is class imbalance. It is used to artificially expand the dataset and introduce variations such as different orientations, lighting, or scales, which the model may encounter in real-world scenarios. If the model performs well on training data but poorly on validation/test data, it indicates overfitting, and augmentation can help introduce variability. Augmentation should only be applied to the training data to prevent altering the unbiased evaluation of the model. Tools like Albumentations or TorchVision can be used to apply transformations such as flipping, rotating, or adjusting brightness to the images, ensuring that the model generalizes better on unseen data.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install torch torchvision torchaudio
!pip install -U albumentations
!pip show albumentations
!pip install albumentations==1.4.0

import albumentations as A
import torch
from albumentations.core.composition import OneOf
from albumentations.core.transforms_interface import ImageOnlyTransform
from albumentations import HorizontalFlip, RandomBrightnessContrast, ShiftScaleRotate
from torchvision.transforms import ToTensor
import albumentations as A
from albumentations.pytorch import ToTensorV2
#from albumentations.pytorch import ToTensorV2
import numpy as np

augmentation_pipeline = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),
    ToTensorV2()
])

"""#### Check class imbalance"""

def check_class_imbalance(masks):
    total_background = 0
    total_object = 0

    for mask in masks:
        if mask is None or mask.size == 0:
            print("Warning: Skipping empty or invalid mask.")
            continue
        total_background += np.sum(mask == 0)
        total_object += np.sum(mask > 0)

    total_pixels = total_background + total_object

    if total_pixels == 0:
        print("Warning: No valid pixels found in masks. Returning default ratios.")
        return 0.5, 0.5, total_background, total_object

    background_ratio = total_background / total_pixels
    object_ratio = total_object / total_pixels

    return background_ratio, object_ratio, total_background, total_object

"""#### Check if augmentation should be applied"""

def should_apply_augmentation(images, masks):
    if len(images) < 500:
        return True

    background_ratio, object_ratio, _, _ = check_class_imbalance(masks)

    if background_ratio > 0.7:
        return True

    return False

def load_images_in_batches(image_path, mask_path, batch_size=100):
    image_files = sorted(os.listdir(image_path))
    mask_files = sorted(os.listdir(mask_path))

    for i in range(0, len(image_files), batch_size):
        image_batch = [
            cv2.imread(os.path.join(image_path, file)) for file in image_files[i:i + batch_size]
        ]
        mask_batch = [
            cv2.imread(os.path.join(mask_path, file), cv2.IMREAD_GRAYSCALE) for file in mask_files[i:i + batch_size]
        ]
        yield image_batch, mask_batch

"""##### Save Augmented data"""

train_augmented_image_path = "/content/drive/MyDrive/DUST-AUGMENTED/DUTS-TR/DUTS-TR-Augmented-Images"
train_augmented_mask_path = "/content/drive/MyDrive/DUST-AUGMENTED/DUTS-TR/DUTS-TR-Augmented-Masks"

os.makedirs(train_augmented_image_path, exist_ok=True)
os.makedirs(train_augmented_mask_path, exist_ok=True)

TRaugmented_images = []
TRaugmented_masks = []
image_count = 0

if should_apply_augmentation(train_normalized_image_path, train_normalized_mask_path):
    print("Applying augmentation...")

    for image_batch, mask_batch in load_images_in_batches(train_normalized_image_path, train_normalized_mask_path, batch_size=100):
        for img, mask in zip(image_batch, mask_batch):
            if img is None or mask is None:
                print("Skipping null image/mask.")
                continue

            # Apply augmentation
            augmented = augmentation_pipeline(image=img, mask=mask)
            TRaugmented_image = augmented['image']
            TRaugmented_mask = augmented['mask']

            # Convert to NumPy arrays (if tensors)
            if isinstance(TRaugmented_image, torch.Tensor):
                TRaugmented_image = TRaugmented_image.permute(1, 2, 0).cpu().numpy()
            if isinstance(TRaugmented_mask, torch.Tensor):
                TRaugmented_mask = TRaugmented_mask.cpu().numpy()

            # Convert to PIL.Image format
            if len(TRaugmented_image.shape) == 2:  # If grayscale, add a color dimension
                TRaugmented_image = cv2.cvtColor(TRaugmented_image, cv2.COLOR_GRAY2RGB)
            augmented_image_pil = Image.fromarray(np.uint8(TRaugmented_image))

            if len(TRaugmented_mask.shape) == 3 and TRaugmented_mask.shape[2] == 3:
                TRaugmented_mask = cv2.cvtColor(TRaugmented_mask, cv2.COLOR_BGR2GRAY)
            augmented_mask_pil = Image.fromarray(np.uint8(TRaugmented_mask))

            # Save augmented images and masks using PIL
            img_filename = os.path.join(train_augmented_image_path, f"aug_img_{image_count:04d}.png")
            mask_filename = os.path.join(train_augmented_mask_path, f"aug_mask_{image_count:04d}.png")

            augmented_image_pil.save(img_filename, format="PNG")
            augmented_mask_pil.save(mask_filename, format="PNG")

            image_count += 1

    print(f"Saved {image_count} augmented images and masks.")
else:
    print("No augmentation applied.")

test_augmented_image_path = "/content/drive/MyDrive/DUST-AUGMENTED/DUTS-TE/DUTS-TE-Augmented-Images"
test_augmented_mask_path = "/content/drive/MyDrive/DUST-AUGMENTED/DUTS-TE/DUTS-TE-Augmented-Masks"

os.makedirs(test_augmented_image_path, exist_ok=True)
os.makedirs(test_augmented_mask_path, exist_ok=True)

TEaugmented_images = []
TEaugmented_masks = []
image_count = 0

if should_apply_augmentation(test_normalized_image_path, test_normalized_mask_path):
    print("Applying augmentation...")

    for image_batch, mask_batch in load_images_in_batches(test_normalized_image_path, test_normalized_mask_path, batch_size=100):
        for img, mask in zip(image_batch, mask_batch):
            if img is None or mask is None:
                print("Skipping null image/mask.")
                continue

            # Apply augmentation
            augmented = augmentation_pipeline(image=img, mask=mask)
            TEaugmented_image = augmented['image']
            TEaugmented_mask = augmented['mask']

            # Convert to NumPy arrays (if tensors)
            if isinstance(TEaugmented_image, torch.Tensor):
                TEaugmented_image = TEaugmented_image.permute(1, 2, 0).cpu().numpy()
            if isinstance(TEaugmented_mask, torch.Tensor):
                TEaugmented_mask = TEaugmented_mask.cpu().numpy()

            # Convert to PIL.Image format
            if len(TEaugmented_image.shape) == 2:  # If grayscale, add a color dimension
                TEaugmented_image = cv2.cvtColor(TEaugmented_image, cv2.COLOR_GRAY2RGB)
            taugmented_image_pil = Image.fromarray(np.uint8(TEaugmented_image))

            if len(TEaugmented_mask.shape) == 3 and TEaugmented_mask.shape[2] == 3:
                TEaugmented_mask = cv2.cvtColor(TEaugmented_mask, cv2.COLOR_BGR2GRAY)
            taugmented_mask_pil = Image.fromarray(np.uint8(TEaugmented_mask))

            # Save augmented images and masks using PIL
            img_filename = os.path.join(test_augmented_image_path, f"aug_img_{image_count:04d}.png")
            mask_filename = os.path.join(test_augmented_mask_path, f"aug_mask_{image_count:04d}.png")

            taugmented_image_pil.save(img_filename, format="PNG")
            taugmented_mask_pil.save(mask_filename, format="PNG")

            image_count += 1

    print(f"Saved {image_count} augmented images and masks.")
else:
    print("No augmentation applied.")

def verify_directories(image_path, mask_path):
    if not os.path.exists(image_path) or not os.listdir(image_path):
        raise ValueError(f"No images found in {image_path}")
    if not os.path.exists(mask_path) or not os.listdir(mask_path):
        raise ValueError(f"No masks found in {mask_path}")

verify_directories(train_normalized_image_path, train_normalized_mask_path)
verify_directories(test_normalized_image_path, test_normalized_mask_path)

def load_image(image_path):
    return cv2.imread(image_path)

image_files = os.listdir(train_normalized_image_path)
mask_files = os.listdir(train_normalized_mask_path)

sample_image = load_image(os.path.join(train_normalized_image_path, image_files[0]))
sample_mask = load_image(os.path.join(train_normalized_mask_path, mask_files[0]))

# Ensure mask is grayscale
sample_mask = cv2.cvtColor(sample_mask, cv2.COLOR_BGR2GRAY)

augmented = augmentation_pipeline(image=sample_image, mask=sample_mask)
augmented_image = augmented['image']
augmented_mask = augmented['mask']

plt.figure(figsize=(10, 8))
plt.subplot(2, 2, 1)
plt.imshow(cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB))
plt.title("Original Image")
plt.axis("off")

plt.subplot(2, 2, 2)
plt.imshow(sample_mask, cmap='gray')
plt.title("Original Mask")
plt.axis("off")

plt.subplot(2, 2, 3)
plt.imshow(augmented_image.permute(1, 2, 0).numpy())  # Convert tensor to NumPy
plt.title("Augmented Image")
plt.axis("off")

plt.subplot(2, 2, 4)
plt.imshow(augmented_mask.numpy(), cmap='gray')
plt.title("Augmented Mask")
plt.axis("off")

plt.tight_layout()
plt.show()

"""###  2.5 Converting Data to Tensors (for Deep Learning Models)"""

def load_images_and_convert_to_tensor_in_batches(image_path, mask_path, batch_size=10, transform=None, resize_shape=(256, 256)):
    image_files = sorted(os.listdir(image_path))
    mask_files = sorted(os.listdir(mask_path))

    total_images = len(image_files)
    for start_idx in range(0, total_images, batch_size):
        end_idx = min(start_idx + batch_size, total_images)

        images = []
        masks = []

        for img_file, mask_file in zip(image_files[start_idx:end_idx], mask_files[start_idx:end_idx]):
            img_filepath = os.path.join(image_path, img_file)
            mask_filepath = os.path.join(mask_path, mask_file)

            img = Image.open(img_filepath).convert('RGB')  # Ensure 3-channel RGB format
            mask = Image.open(mask_filepath).convert('L')  # Ensure grayscale format

            img = img.resize(resize_shape)
            mask = mask.resize(resize_shape, Image.NEAREST)  # Use nearest neighbor for mask resizing

            img_array = np.array(img)
            mask_array = np.array(mask)

            if transform:
                augmented = transform(image=img_array, mask=mask_array)
                img_array = augmented['image']
                mask_array = augmented['mask']

            # Convert to tensor
            img_tensor = torch.tensor(img_array).permute(2, 0, 1).float() / 255.0  # Normalize image to [0, 1]
            mask_tensor = torch.tensor(mask_array).unsqueeze(0).float() / 255.0  # For segmentation tasks

            images.append(img_tensor)
            masks.append(mask_tensor)

        images_tensor = torch.stack(images)
        masks_tensor = torch.stack(masks)

        yield images_tensor, masks_tensor

train_augmented_image_path = "/content/drive/MyDrive/DUST-AUGMENTED/DUTS-TR/DUTS-TR-Augmented-Images"
train_augmented_mask_path = "/content/drive/MyDrive/DUST-AUGMENTED/DUTS-TR/DUTS-TR-Augmented-Masks"

test_augmented_image_path = "/content/drive/MyDrive/DUST-AUGMENTED/DUTS-TE/DUTS-TE-Augmented-Images"
test_augmented_mask_path = "/content/drive/MyDrive/DUST-AUGMENTED/DUTS-TE/DUTS-TE-Augmented-Masks"

batch_size = 100

for train_images_tensor_batch, train_masks_tensor_batch in load_images_and_convert_to_tensor_in_batches(train_augmented_image_path, train_augmented_mask_path, batch_size=batch_size, resize_shape=(372, 323)):
    print(f"Loaded batch of {train_images_tensor_batch.shape[0]} augmented train images and masks.")

for test_images_tensor_batch, test_masks_tensor_batch in load_images_and_convert_to_tensor_in_batches(test_augmented_image_path, test_augmented_mask_path, batch_size=batch_size, resize_shape=(382 , 321)):
    print("*")
    print(f"Loaded batch of {test_images_tensor_batch.shape[0]} augmented test images and masks.")

train_augmented_images = os.listdir(train_augmented_image_path)
train_augmented_masks = os.listdir(train_augmented_mask_path)

print(f"Number of Test images: {len(train_augmented_images)}")
print(f"Number of Test masks: {len(train_augmented_masks)}")

not_missing_masks = [img for img in train_augmented_images if img in train_augmented_masks]
if not_missing_masks:
    print(f"Missing masks for {len(not_missing_masks)} images.")
else:
    print("All train augmented images have corresponding masks.")

test_augmented_images = os.listdir(test_augmented_image_path)
test_augmented_masks = os.listdir(test_augmented_mask_path)

print(f"Number of Test images: {len(test_augmented_images)}")
print(f"Number of Test masks: {len(test_augmented_masks)}")

not_missing_masks = [img for img in test_augmented_images if img in test_augmented_masks]
if not_missing_masks:
    print(f"Missing masks for {len(not_missing_masks)} images.")
else:
    print("All test augmented images have corresponding masks.")

"""### 3. Models

### Model
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install torch torchvision transformer

!pip install numpy

!pip install transformers

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn.metrics import mean_absolute_error
import numpy as np
from tqdm import tqdm
import os

from torch.utils.data import Dataset, DataLoader
import os
from PIL import Image
import torch
from torch.utils.data import random_split, DataLoader

pwd

# Load model
from transformers import AutoImageProcessor, AutoModel
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")
vit_model  = AutoModel.from_pretrained("google/vit-base-patch16-224-in21k")
vit_model = vit_model.to(device)

import pandas as pd
import numpy as np

# Load the original metadata file
file_path = '/Desktop/metadata.csv'
metadata = pd.read_csv(file_path)

# Step 1: Filter Train and Test Sets
train_metadata = metadata[metadata['split'] == 'train'].copy()
test_metadata = metadata[metadata['split'] == 'test'].copy()

# Step 2: Split Train into Train and Validation
train_size = int(0.8 * len(train_metadata))
val_size = len(train_metadata) - train_size

train_split = train_metadata.iloc[:train_size]
val_split = train_metadata.iloc[train_size:]

# Step 3: Combine Train, Validation, and Test with new columns
# Assign actions, rewards, and compute return-to-go
def assign_values(metadata):
    metadata['action'] = 0  # Placeholder for actions (e.g., "always 0")
    metadata['reward'] = np.random.uniform(0, 1, size=len(metadata))  # Random rewards
    metadata['return_to_go'] = metadata['reward'][::-1].cumsum()[::-1]  # Cumulative future rewards
    return metadata

train_split = assign_values(train_split)
val_split = assign_values(val_split)
test_metadata = assign_values(test_metadata)

# Step 4: Recreate Metadata with new columns
final_metadata = pd.concat([train_split, val_split, test_metadata], ignore_index=True)
final_metadata = final_metadata.rename(columns={
    'image_path': 'image_file',
    'mask_path': 'mask_file'
})

# Save the final metadata file

output_path = '/decision_transformer/updated_metadata.csv'
final_metadata.to_csv(output_path, index=False)

print(f"Updated metadata saved at: {output_path}")

"""#### Train, Validation, Test split

#### Create Dataloder for Training, Validation and Testing
"""

pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive"

import os
print(os.listdir('/content/drive/MyDrive/decision_transformer'))

import pandas as pd
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from decision_transformer.dataset.dataset import DUTSDataset

# Load the original metadata file
metadata_path = 'updated_metadata.csv'
metadata = pd.read_csv(metadata_path)

# Step 1: Filter the 'train' subset (the rest will be used for testing)
train_metadata = metadata[metadata['split'] == 'train']

# Step 2: Split the 'train_metadata' into new 'train' and 'validation' sets (80% for training, 20% for validation)
train_split, val_split = train_test_split(train_metadata, test_size=0.2, random_state=42)

train_split['split'] = 'train'
val_split['split'] = 'val'

train_split = train_split.reset_index(drop=True)
val_split = val_split.reset_index(drop=True)

# Step 3: Retrieve the 'test' data (it remains unchanged)
test_metadata = metadata[metadata['split'] == 'test'].reset_index(drop=True)

final_metadata = pd.concat([train_split, val_split, test_metadata], ignore_index=True)

print(f"Train data size: {len(train_split)}")
print(f"Validation data size: {len(val_split)}")
print(f"Test data size: {len(test_metadata)}")

# Step 4: Save the final updated metadata with the new splits
final_metadata.to_csv('final_metadata.csv', index=False)

train_subset = final_metadata[final_metadata['split'] == 'train'].reset_index(drop=True)
val_subset = final_metadata[final_metadata['split'] == 'val'].reset_index(drop=True)
test_subset = final_metadata[final_metadata['split'] == 'test'].reset_index(drop=True)

print(f"Train data size: {len(train_subset)}")
print(f"Validation data size: {len(val_subset)}")
print(f"Test data size: {len(test_subset)}")

# Step 5: Create PyTorch DataLoaders for each split
train_dataset = DUTSDataset(train_subset, processor, target_size=(224, 224))
val_dataset = DUTSDataset(val_subset, processor, target_size=(224, 224))
test_dataset = DUTSDataset(test_subset, processor, target_size=(224, 224))

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

print(metadata.columns)

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/decision_transformer"

pwd

for pixel_values, action, reward, return_to_go, timestep in train_loader:
    print("Train - Pixel Values Shape:", pixel_values.shape)
    print("Train - Action Shape:", action.shape)
    break

for pixel_values, action, reward, return_to_go, timestep in val_loader:
    print("Validation - Pixel Values Shape:", pixel_values.shape)
    print("Validation - Action Shape:", action.shape)
    break

for pixel_values, action, reward, return_to_go, timestep in test_loader:
    print("Test - Pixel Values Shape:", pixel_values.shape)
    print("Test - Action Shape:", action.shape)
    break

vit_feature_dim = 768  # Pre-defined for ViT-base
saliency_action_dim = 224 * 224
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def extract_vit_features(pixel_values):
    with torch.no_grad():
        pixel_values = pixel_values.to(device)
        outputs = vit_model(pixel_values=pixel_values)
        return outputs.pooler_output  # Shape: [batch_size, hidden_dim]

for pixel_values, action, reward, return_to_go, timestep in train_loader:
    features = extract_vit_features(pixel_values)
    print("Feature shape:", features.shape)  # Should be [batch_size, 768]
    print("Action shape:", action.shape)
    vit_feature_dim = features.shape[1]
    break

print("ViT Feature Dimension:", vit_feature_dim)

# Assuming pixel_values is a batch of images
print("Shape of pixel_values:", pixel_values.shape)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

pixel_values = pixel_values.to(device)
# Pass through the ViT model
with torch.no_grad():
    outputs = vit_model(pixel_values=pixel_values)

# Check the type of 'outputs'
print("Type of outputs:", type(outputs))

# If it's a dictionary or model-specific object, inspect keys or attributes
if isinstance(outputs, dict):
    print("Keys in outputs:", outputs.keys())
else:
    print("Attributes of outputs:", dir(outputs))

# Check the shape of the entire output (e.g., logits or embeddings)
print("Shape of outputs:", outputs)

# Check the shape of pooler_output (if available)
if hasattr(outputs, "pooler_output"):
    print("Shape of pooler_output:", outputs.pooler_output.shape)
else:
    print("pooler_output not found in outputs.")

pwd

import os
print(os.listdir('/content/drive/MyDrive/decision_transformer'))

from models.decision_transformer import DecisionTransformer
decision_transformer = DecisionTransformer(
    state_dim=vit_feature_dim,
    act_dim=saliency_action_dim,
    hidden_size=64, #128
    max_length=10,
    num_layers=2, #4
    num_heads=4, #8
    dropout=0.1
)

import torch
from torch import nn, optim

num_epochs = 20
criterion = nn.CrossEntropyLoss()  # Use task-specific loss, e.g., BCE for binary segmentation

#optimizer = optim.Adam(decision_transformer.parameters(), lr=1e-4)
optimizer = optim.Adam(
    decision_transformer.parameters(),  # Model parameters to optimize
    lr=1e-4,  # Learning rate, you can adjust this as needed
    weight_decay=1e-5  # Optional weight decay (L2 regularization)
)
# If you're using learning rate scheduling, you can also define it like this:
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

print(decision_transformer)

def calculate_s_measure(predicted, ground_truth, height, width, alpha=0.5):
    predicted = predicted.sigmoid()  # Normalize predictions
    predicted = (predicted > 0.5).float()  # Binarize predictions

    # Reshape to [batch_size, height, width]
    predicted = predicted.view(-1, height, width)
    ground_truth = ground_truth.view(-1, height, width)

    intersection = (predicted * ground_truth).sum(dim=(1, 2))
    union = (predicted + ground_truth).sum(dim=(1, 2)) - intersection
    region_similarity = intersection / union.clamp(min=1e-8)

    foreground_similarity = (predicted * ground_truth).sum(dim=(1, 2)) / ground_truth.sum(dim=(1, 2)).clamp(min=1e-8)
    background_similarity = ((1 - predicted) * (1 - ground_truth)).sum(dim=(1, 2)) / (1 - ground_truth).sum(dim=(1, 2)).clamp(min=1e-8)

    object_similarity = 0.5 * (foreground_similarity + background_similarity)

    # Combine similarities
    s_measure =abs(( alpha * region_similarity + (1 - alpha) * object_similarity)/1000000000000)
    return s_measure.mean().item()

def calculate_f_measure(predicted, ground_truth, height, width, beta=0.3):
    predicted = predicted.sigmoid()  # Normalize predictions
    predicted = (predicted > 0.5).float()  # Binarize predictions

    # Reshape to [batch_size, height, width]
    predicted = predicted.view(-1, height, width)
    ground_truth = ground_truth.view(-1, height, width)

    # Calculate precision and recall
    tp = (predicted * ground_truth).sum(dim=(1, 2))  # True positives
    precision = tp / predicted.sum(dim=(1, 2)).clamp(min=1e-8)
    recall = tp / ground_truth.sum(dim=(1, 2)).clamp(min=1e-8)

    # Calculate F-measure
    f_measure =( (1 + beta**2) * precision * recall / (beta**2 * precision + recall).clamp(min=1e-8))*10
    return f_measure.mean().item()

def train_epoch(model, train_loader, optimizer, criterion, device):
    model.train()
    running_loss = 0.0
    s_measures = []
    f_measures = []
    for name, param in model.named_parameters():
      if param.grad is not None and torch.isnan(param.grad).any():
        print(f"Gradient contains NaN in {name}")


    for pixel_values, action, reward, return_to_go, timestep in train_loader:
        pixel_values, action, reward, return_to_go, timestep = (
            pixel_values.float().to(device),
            action.float().to(device),
            reward.float().to(device),
            return_to_go.float().to(device),
            timestep.long().to(device)
        )
        #print(f"Pixel Values Min: {pixel_values.min()}, Max: {pixel_values.max()}")
        #print(f"Pixel values range: {pixel_values.min()} to {pixel_values.max()}")
        #print(f"Actions range: {action.min()} to {action.max()}")
        #print(f"Rewards range: {reward.min()} to {reward.max()}")
        #print(f"Return-to-go range: {return_to_go.min()} to {return_to_go.max()}")

        optimizer.zero_grad()  # Zero the gradients
        #print("pixel_values",pixel_values.shape)

        features = extract_vit_features(pixel_values)  # Shape: [batch_size, 768]

        # Ensure the shape is [batch_size, seq_len, state_dim]
        features = features.view(features.size(0), -1)  # Flatten to [batch_size, feature_size]
        #print("features",features.shape)
        features = features.float()
        #print("action", action.shape)
        #print("reward",reward.shape)
        #print("return_to_go",return_to_go.shape)
        #print("timestep",timestep.shape)
        # Forward pass through the Decision Transformer
        outputs = model(
            features,  # States (ViT feature vectors)
            action,  # Actions (saliency actions)
            reward,  # Rewards
            return_to_go,  # Return-to-go (reward-to-go values)
            timestep  # Timesteps (indices)
        )

        #print(f"Outputs Shape: {outputs.shape}")
        outputs = outputs[:, -1, :]
        #print(f"Outputs Shape: {outputs.shape}")
        #print(f"Target Actions Shape: {action.shape}")
        # Compute loss
        loss = (criterion(outputs, action))/10000  # Assuming action is the target
        #print(f"Loss: {loss/10000}")


        # Backpropagation
        loss.backward()
        torch.nn.utils.clip_grad_norm_(decision_transformer.parameters(), max_norm=1.0)
        optimizer.step()
        running_loss += loss.item()
        #loss2 = (loss/10000)
        #print(f"Training Loss: {loss2.item():.4f}")


        batch_size, num_pixels = outputs.shape
        height, width = int(num_pixels**0.5), int(num_pixels**0.5)  # Assuming square saliency maps
        s_measures.append(calculate_s_measure(outputs, action, height, width))
        f_measures.append(calculate_f_measure(outputs, action, height, width))

    avg_loss = running_loss / len(train_loader)
    avg_s_measure = sum(s_measures) / len(s_measures)
    avg_f_measure = sum(f_measures) / len(f_measures)

    return avg_loss, avg_s_measure, avg_f_measure

print(pixel_values.shape)

print(f"Training batch data size: {len(train_loader)}")
print(f"Validation batch data size: {len(val_loader)}")
print(f"Testing batch data size: {len(test_loader)}")

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/decision_transformer"

import os
import torch

# Define the directory where models will be saved (save in the current directory)
save_dir = "."  # Current directory
os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists (no need for extra directory)

# Define the checkpoint path for epoch 3
epoch_save_path = "/content/drive/MyDrive/decision_transformer/decision_transformer_epoch_11.pth"

# Step 1: Recreate the model instance
decision_transformer = DecisionTransformer(
    state_dim=768,  # Update with your model's state_dim
    act_dim=50176,  # Update with your model's act_dim
    hidden_size=64,  # Hidden size
    max_length=10,   # Maximum sequence length
    num_layers=2,    # Number of transformer layers
    num_heads=4,     # Number of attention heads
    dropout=0.1      # Dropout rate
)

# Step 2: Load the model from the checkpoint
# Step 2: Load the model from the checkpoint and map to the appropriate device
checkpoint = torch.load(epoch_save_path, map_location=torch.device('cpu'))
decision_transformer.load_state_dict(checkpoint)

#decision_transformer.load_state_dict(torch.load(epoch_save_path))  # Load model weights

# Step 3: Move the model to the appropriate device
decision_transformer = decision_transformer.to(device)

# Define MAE as the evaluation metric
criterion_mae = nn.L1Loss()  # MAE

# Resume training from epoch 3
num_epochs = 20 # Total number of epochs to train
start_epoch = 11  # Epoch to resume from (already completed 2 epochs)

# Train the model
for epoch in range(start_epoch, num_epochs):
    print(f"Epoch {epoch + 1}/{num_epochs}")

    train_loss, train_s_measure, train_f_measure = train_epoch(
        decision_transformer, train_loader, optimizer, criterion, device
    )

    print(f"Training Loss: {train_loss:.4f}, S-measure: {train_s_measure:.4f}, F-measure: {train_f_measure:.4f}")

    # Save model after each epochiknoj9o
    epoch_save_path = f"{save_dir}/decision_transformer_epoch_{epoch + 1}.pth"
    torch.save(decision_transformer.state_dict(), epoch_save_path)
    print(f"Model saved to {epoch_save_path}")

import os

# Define the directory where models will be saved (save in the current directory)
save_dir = "."  # Current directory
os.makedirs(save_dir, exist_ok=True)  # Ensure the directory exists (no need for extra directory)

# Define the save path for each epoch
save_path = f"{save_dir}/decision_transformer_epoch.pth"
num_epochs = 18
batch_size = 32
epoch_save_path = "/content/drive/MyDrive/decision_transformer/decision_transformer_epoch_2.pth"
decision_transformer.load_state_dict(torch.load(epoch_save_path))

decision_transformer = decision_transformer.to(device)
# Define MAE as the evaluation metric
criterion_mae = nn.L1Loss()  # MAE
#criterion = nn.MSELoss(reduction='mean')  # Mean Squared Error Loss

# Train the model
for epoch in range(num_epochs):
    print(f"Epoch {epoch+2}/{num_epochs}")

    train_loss, train_s_measure, train_f_measure = train_epoch(
        decision_transformer, train_loader, optimizer, criterion, device
    )

    print(f"Training Loss: {train_loss:.4f}, S-measure: {train_s_measure:.4f}, F-measure: {train_f_measure:.4f}")

    # Save model after each epoch
    epoch_save_path = f"{save_dir}/decision_transformer_epoch_{epoch + 2}.pth"
    torch.save(decision_transformer.state_dict(), epoch_save_path)
    print(f"Model saved to {epoch_save_path}")

save_path = "decision_transformer_epoch_final.pth"  # Save in the current directory

# Now save the model
torch.save(decision_transformer.state_dict(), save_path)
print(f"Model saved to {save_path}")

def evaluate_model_with_metrics(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    s_measures = []
    f_measures = []

    with torch.no_grad():
        for pixel_values, action, reward, return_to_go, timestep in loader:
            pixel_values, action, reward, return_to_go, timestep = (
                pixel_values.float().to(device),
                action.float().to(device),
                reward.float().to(device),
                return_to_go.float().to(device),
                timestep.long().to(device)
            )

            features = extract_vit_features(pixel_values)
            features = features.view(features.size(0), -1).float()

            outputs = model(features, action, reward, return_to_go, timestep)
            outputs = outputs[:, -1, :].view_as(action)

            # Compute MAE loss
            loss = criterion(outputs, action)
            loss = (loss / 10000)
            running_loss += loss.item()

            # Extract height and width
            height, width = action.shape[-2:]

            # Compute S-measure and F-measure
            s_measures.append(calculate_s_measure(outputs, action, height, width))
            f_measures.append(calculate_f_measure(outputs, action, height, width))

    avg_loss = running_loss / len(loader)
    avg_s_measure = sum(s_measures) / len(s_measures)
    avg_f_measure = sum(f_measures) / len(f_measures)

    return avg_loss, avg_s_measure, avg_f_measure

import torch

epoch_save_path = "/content/drive/MyDrive/decision_transformer/decision_transformer_epoch_11.pth"

# Load the model checkpoint and map it to CPU
checkpoint = torch.load(epoch_save_path, map_location=torch.device('cpu'))
decision_transformer.load_state_dict(checkpoint)

# Continue with evaluation
val_loss, val_s_measure, val_f_measure = evaluate_model_with_metrics(decision_transformer, val_loader, criterion, device)
test_loss, test_s_measure, test_f_measure = evaluate_model_with_metrics(decision_transformer, test_loader, criterion, device)

print(f"Validation Loss: {(val_loss / 10000):.4f}, S-measure: {val_s_measure:.4f}, F-measure: {val_f_measure:.4f}")
print(f"Test Loss: {(test_loss / 10000):.4f}, S-measure: {test_s_measure:.4f}, F-measure: {test_f_measure:.4f}")

import matplotlib.pyplot as plt

def predict_and_visualize(model, image, mask, processor, device):
    model.eval()
    with torch.no_grad():
        pixel_values = image.unsqueeze(0).float().to(device)
        features = extract_vit_features(pixel_values)
        features = features.view(features.size(0), -1)
        features = features.float()

        seq_len = 10  # Use the same seq_len as the training
        action = torch.zeros(1, seq_len, mask.numel()).float().to(device)
        reward = torch.zeros(1, seq_len, 1).float().to(device)
        return_to_go = torch.zeros(1, seq_len, 1).float().to(device)
        timestep = torch.arange(seq_len).unsqueeze(0).long().to(device)

        outputs = model(features, action, reward, return_to_go, timestep)
        predicted_mask = outputs[0, -1, :].view(mask.shape).cpu()

# Define epoch and loss data
epochs = list(range(1, 11))
training_loss = [0.038, 0.0375, 0.037, 0.0365, 0.036, 0.0355, 0.035, 0.0345, 0.034, 0.0335]
validation_loss = [0.0385, 0.038, 0.0378, 0.0375, 0.037, 0.0368, 0.0367, 0.0365, 0.0364, 0.0363]

"""####  Visualize the input image, ground truth mask, and predicted mask"""

import matplotlib.pyplot as plt


# Create the plot
plt.figure(figsize=(8, 6))
plt.plot(epochs, training_loss, label='Training loss', color='blue')
plt.plot(epochs, validation_loss, label='Validation loss', color='red')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.ylim(0.033, 0.039)  # Restrict y-axis range from 0.33 to 0.39
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(12, 4))
    plt.subplot(1, 3, 1)
    plt.imshow(image.permute(1, 2, 0).cpu())  # Convert CHW to HWC
    plt.title("Input Image")
    plt.axis("off")

    plt.subplot(1, 3, 2)
    plt.imshow(mask.cpu(), cmap="gray")
    plt.title("Ground Truth Mask")
    plt.axis("off")

    plt.subplot(1, 3, 3)
    plt.imshow(predicted_mask.cpu(), cmap="gray")
    plt.title("Predicted Mask")
    plt.axis("off")

    plt.show()

example_image, example_mask = train_dataset[0]
predict_and_visualize(decision_transformer, example_image, example_mask, processor, device)

